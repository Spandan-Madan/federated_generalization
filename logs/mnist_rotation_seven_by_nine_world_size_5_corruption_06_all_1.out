/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Corrupting the following ranks:
[1, 5]
Building train + in-distribution test data loader from mnist_rotation_seven_by_nine
Building OOD test data loader from mnist_rotation_nine_by_nine
loaders done, starting training...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Worker rank 1 initializing RPC
[W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[W tensorpipe_agent.cpp:180] Failed to look up the IP address for the hostname (EAI_NONAME: unknown node or service (this error originated at tensorpipe/transport/uv/utility.cc:97)), defaulting to 127.0.0.1
Worker 1 done initializing RPC
saving as /Users/spandanmadan/saved_models/mnist_rotation_seven_by_nine_world_size_5_corruption_06_all_rank_1.pt
Starting Epoch:0
Rank 1 training batch 0 loss 2.6127748489379883
Rank 1 training batch 5 loss 2.3379836082458496
Rank 1 training batch 10 loss 2.2537412643432617
Rank 1 training batch 15 loss 2.2185885906219482
Rank 1 training batch 20 loss 2.193167209625244
Rank 1 training batch 25 loss 2.0096449851989746
Rank 1 training batch 30 loss 2.0716004371643066
Rank 1 training batch 35 loss 1.9713271856307983
Rank 1 training batch 40 loss 1.912133812904358
Rank 1 training batch 45 loss 1.7931792736053467
Rank 1 training batch 50 loss 1.8300014734268188
Rank 1 training batch 55 loss 1.801811933517456
Rank 1 training batch 60 loss 1.6333523988723755
Rank 1 training batch 65 loss 1.7604986429214478
Rank 1 training batch 70 loss 1.7235106229782104
Rank 1 training batch 75 loss 1.5962448120117188
Rank 1 training batch 80 loss 1.692360520362854
Rank 1 training batch 85 loss 1.5750153064727783
Rank 1 training batch 90 loss 1.7140426635742188
Rank 1 training batch 95 loss 1.548491358757019
Rank 1 training batch 100 loss 1.5202114582061768
Rank 1 training batch 105 loss 1.4852856397628784
Rank 1 training batch 110 loss 1.4291675090789795
Rank 1 training batch 115 loss 1.5152778625488281
Rank 1 training batch 120 loss 1.3965890407562256
Rank 1 training batch 125 loss 1.253035545349121
Rank 1 training batch 130 loss 1.3470836877822876
Rank 1 training batch 135 loss 1.2947250604629517
Rank 1 training batch 140 loss 1.3687885999679565
Rank 1 training batch 145 loss 1.1590194702148438
Rank 1 training batch 150 loss 1.3152146339416504
Rank 1 training batch 155 loss 1.2058204412460327
Rank 1 training batch 160 loss 1.1785190105438232
Rank 1 training batch 165 loss 1.1939126253128052
Rank 1 training batch 170 loss 1.146109938621521
Rank 1 training batch 175 loss 1.1638778448104858
Rank 1 training batch 180 loss 1.123775839805603
Rank 1 training batch 185 loss 1.0873699188232422
Rank 1 training batch 190 loss 1.0487536191940308
Rank 1 training batch 195 loss 1.2625133991241455
Rank 1 training batch 200 loss 1.050916314125061
Rank 1 training batch 205 loss 1.0688954591751099
Rank 1 training batch 210 loss 1.1579910516738892
Rank 1 training batch 215 loss 1.0575168132781982
Rank 1 training batch 220 loss 1.0594491958618164
Rank 1 training batch 225 loss 1.0562649965286255
Rank 1 training batch 230 loss 0.9414781928062439
Rank 1 training batch 235 loss 1.101906657218933
Rank 1 training batch 240 loss 1.0651423931121826
Rank 1 training batch 245 loss 0.9756121635437012
Rank 1 training batch 250 loss 0.9713690280914307
Rank 1 training batch 255 loss 1.1889652013778687
Rank 1 training batch 260 loss 0.9491648077964783
Rank 1 training batch 265 loss 0.9519898295402527
Rank 1 training batch 270 loss 0.7716248035430908
Rank 1 training batch 275 loss 0.9627696871757507
Rank 1 training batch 280 loss 0.9223813414573669
Rank 1 training batch 285 loss 0.9400581121444702
Rank 1 training batch 290 loss 0.8451154232025146
Rank 1 training batch 295 loss 1.0016758441925049
Rank 1 training batch 300 loss 0.8363126516342163
Rank 1 training batch 305 loss 0.767093300819397
Rank 1 training batch 310 loss 0.7249627709388733
Rank 1 training batch 315 loss 0.8297563195228577
Rank 1 training batch 320 loss 0.7358717322349548
Rank 1 training batch 325 loss 0.8272623419761658
Rank 1 training batch 330 loss 0.7948916554450989
Rank 1 training batch 335 loss 0.736437976360321
Rank 1 training batch 340 loss 0.6801006197929382
Rank 1 training batch 345 loss 0.8961379528045654
Rank 1 training batch 350 loss 0.8011354207992554
Rank 1 training batch 355 loss 0.7745943665504456
Rank 1 training batch 360 loss 0.8703029155731201
Rank 1 training batch 365 loss 0.5885439515113831
Rank 1 training batch 370 loss 0.8811998963356018
Rank 1 training batch 375 loss 0.6857534050941467
Rank 1 training batch 380 loss 0.7813503742218018
Rank 1 training batch 385 loss 0.7521026730537415
Rank 1 training batch 390 loss 0.7378140091896057
Rank 1 training batch 395 loss 0.5970282554626465
Rank 1 training batch 400 loss 0.6180742383003235
Rank 1 training batch 405 loss 0.6962307691574097
Rank 1 training batch 410 loss 0.7564986944198608
Rank 1 training batch 415 loss 0.5164093375205994
Rank 1 training batch 420 loss 0.798770010471344
Rank 1 training batch 425 loss 0.750816822052002
Rank 1 training batch 430 loss 0.6677120923995972
Rank 1 training batch 435 loss 0.6942931413650513
Rank 1 training batch 440 loss 0.6056321859359741
Rank 1 training batch 445 loss 0.691824197769165
Rank 1 training batch 450 loss 0.7629825472831726
Rank 1 training batch 455 loss 0.6682600975036621
Rank 1 training batch 460 loss 0.5818713307380676
Rank 1 training batch 465 loss 0.7661795616149902
Rank 1 training batch 470 loss 0.599784255027771
Rank 1 training batch 475 loss 0.5964738726615906
Rank 1 training batch 480 loss 0.6600712537765503
Rank 1 training batch 485 loss 0.5332319140434265
Rank 1 training batch 490 loss 0.6059438586235046
Rank 1 training batch 495 loss 0.6231813430786133
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.8073
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.4184
Starting Epoch:1
Rank 1 training batch 0 loss 0.5063512325286865
Rank 1 training batch 5 loss 0.6556398272514343
Rank 1 training batch 10 loss 0.5906073451042175
Rank 1 training batch 15 loss 0.6526050567626953
Rank 1 training batch 20 loss 0.5232689380645752
Rank 1 training batch 25 loss 0.5486392974853516
Rank 1 training batch 30 loss 0.5113539099693298
Rank 1 training batch 35 loss 0.4535583555698395
Rank 1 training batch 40 loss 0.41720494627952576
Rank 1 training batch 45 loss 0.5207775831222534
Rank 1 training batch 50 loss 0.5046159029006958
Rank 1 training batch 55 loss 0.5818091034889221
Rank 1 training batch 60 loss 0.5769245028495789
Rank 1 training batch 65 loss 0.6627126932144165
Rank 1 training batch 70 loss 0.6266438961029053
Rank 1 training batch 75 loss 0.4375416040420532
Rank 1 training batch 80 loss 0.6138253211975098
Rank 1 training batch 85 loss 0.5695806741714478
Rank 1 training batch 90 loss 0.48637717962265015
Rank 1 training batch 95 loss 0.48842135071754456
Rank 1 training batch 100 loss 0.4686340093612671
Rank 1 training batch 105 loss 0.4762624502182007
Rank 1 training batch 110 loss 0.5369466543197632
Rank 1 training batch 115 loss 0.5470627546310425
Rank 1 training batch 120 loss 0.43049100041389465
Rank 1 training batch 125 loss 0.6254728436470032
Rank 1 training batch 130 loss 0.4998258650302887
Rank 1 training batch 135 loss 0.42429396510124207
Rank 1 training batch 140 loss 0.41241589188575745
Rank 1 training batch 145 loss 0.41977354884147644
Rank 1 training batch 150 loss 0.3898971974849701
Rank 1 training batch 155 loss 0.39223191142082214
Rank 1 training batch 160 loss 0.40554511547088623
Rank 1 training batch 165 loss 0.39253002405166626
Rank 1 training batch 170 loss 0.35865333676338196
Rank 1 training batch 175 loss 0.29353392124176025
Rank 1 training batch 180 loss 0.435120552778244
Rank 1 training batch 185 loss 0.45558789372444153
Rank 1 training batch 190 loss 0.459267795085907
Rank 1 training batch 195 loss 0.5148935914039612
Rank 1 training batch 200 loss 0.4493115246295929
Rank 1 training batch 205 loss 0.3676546812057495
Rank 1 training batch 210 loss 0.5439733266830444
Rank 1 training batch 215 loss 0.44009286165237427
Rank 1 training batch 220 loss 0.4648808538913727
Rank 1 training batch 225 loss 0.3825995624065399
Rank 1 training batch 230 loss 0.455567330121994
Rank 1 training batch 235 loss 0.38308480381965637
Rank 1 training batch 240 loss 0.34247738122940063
Rank 1 training batch 245 loss 0.4089246690273285
Rank 1 training batch 250 loss 0.3401339054107666
Rank 1 training batch 255 loss 0.4552291929721832
Rank 1 training batch 260 loss 0.4143536388874054
Rank 1 training batch 265 loss 0.3146311342716217
Rank 1 training batch 270 loss 0.4718755781650543
Rank 1 training batch 275 loss 0.5157449841499329
Rank 1 training batch 280 loss 0.45682254433631897
Rank 1 training batch 285 loss 0.3233276605606079
Rank 1 training batch 290 loss 0.4687775671482086
Rank 1 training batch 295 loss 0.39150193333625793
Rank 1 training batch 300 loss 0.3945402503013611
Rank 1 training batch 305 loss 0.2682478427886963
Rank 1 training batch 310 loss 0.33632391691207886
Rank 1 training batch 315 loss 0.35052117705345154
Rank 1 training batch 320 loss 0.3619145452976227
Rank 1 training batch 325 loss 0.2645517587661743
Rank 1 training batch 330 loss 0.3569505214691162
Rank 1 training batch 335 loss 0.427998811006546
Rank 1 training batch 340 loss 0.38811194896698
Rank 1 training batch 345 loss 0.5113252401351929
Rank 1 training batch 350 loss 0.32887476682662964
Rank 1 training batch 355 loss 0.41456812620162964
Rank 1 training batch 360 loss 0.36337098479270935
Rank 1 training batch 365 loss 0.3867509961128235
Rank 1 training batch 370 loss 0.3480142652988434
Rank 1 training batch 375 loss 0.3779822289943695
Rank 1 training batch 380 loss 0.5438293814659119
Rank 1 training batch 385 loss 0.44763004779815674
Rank 1 training batch 390 loss 0.3869999945163727
Rank 1 training batch 395 loss 0.28672757744789124
Rank 1 training batch 400 loss 0.48111072182655334
Rank 1 training batch 405 loss 0.376285195350647
Rank 1 training batch 410 loss 0.4177643060684204
Rank 1 training batch 415 loss 0.4104752838611603
Rank 1 training batch 420 loss 0.312780499458313
Rank 1 training batch 425 loss 0.32737594842910767
Rank 1 training batch 430 loss 0.28075480461120605
Rank 1 training batch 435 loss 0.31362444162368774
Rank 1 training batch 440 loss 0.33656924962997437
Rank 1 training batch 445 loss 0.25900325179100037
Rank 1 training batch 450 loss 0.2957930266857147
Rank 1 training batch 455 loss 0.3361610174179077
Rank 1 training batch 460 loss 0.3007183372974396
Rank 1 training batch 465 loss 0.37900763750076294
Rank 1 training batch 470 loss 0.3791086971759796
Rank 1 training batch 475 loss 0.3839893639087677
Rank 1 training batch 480 loss 0.23428155481815338
Rank 1 training batch 485 loss 0.16781648993492126
Rank 1 training batch 490 loss 0.29314735531806946
Rank 1 training batch 495 loss 0.3017963469028473
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.8895
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.4868
Starting Epoch:2
Rank 1 training batch 0 loss 0.3242674767971039
Rank 1 training batch 5 loss 0.18702763319015503
Rank 1 training batch 10 loss 0.3431577682495117
Rank 1 training batch 15 loss 0.30169737339019775
Rank 1 training batch 20 loss 0.3244576156139374
Rank 1 training batch 25 loss 0.3173999488353729
Rank 1 training batch 30 loss 0.18816496431827545
Rank 1 training batch 35 loss 0.2841796278953552
Rank 1 training batch 40 loss 0.24293942749500275
Rank 1 training batch 45 loss 0.29674097895622253
Rank 1 training batch 50 loss 0.4116687774658203
Rank 1 training batch 55 loss 0.2047860324382782
Rank 1 training batch 60 loss 0.26972371339797974
Rank 1 training batch 65 loss 0.27578312158584595
Rank 1 training batch 70 loss 0.2611462473869324
Rank 1 training batch 75 loss 0.21633271872997284
Rank 1 training batch 80 loss 0.21492889523506165
Rank 1 training batch 85 loss 0.2870093882083893
Rank 1 training batch 90 loss 0.2307034134864807
Rank 1 training batch 95 loss 0.2149139940738678
Rank 1 training batch 100 loss 0.27255362272262573
Rank 1 training batch 105 loss 0.23052135109901428
Rank 1 training batch 110 loss 0.22055307030677795
Rank 1 training batch 115 loss 0.21766208112239838
Rank 1 training batch 120 loss 0.32635819911956787
Rank 1 training batch 125 loss 0.3184099793434143
Rank 1 training batch 130 loss 0.39312824606895447
Rank 1 training batch 135 loss 0.2893095016479492
Rank 1 training batch 140 loss 0.22272032499313354
Rank 1 training batch 145 loss 0.36669227480888367
Rank 1 training batch 150 loss 0.3598201274871826
Rank 1 training batch 155 loss 0.24653558433055878
Rank 1 training batch 160 loss 0.34033843874931335
Rank 1 training batch 165 loss 0.2757730782032013
Rank 1 training batch 170 loss 0.2060556560754776
Rank 1 training batch 175 loss 0.2631882429122925
Rank 1 training batch 180 loss 0.25528448820114136
Rank 1 training batch 185 loss 0.2969433069229126
Rank 1 training batch 190 loss 0.2563343048095703
Rank 1 training batch 195 loss 0.24064704775810242
Rank 1 training batch 200 loss 0.27996060252189636
Rank 1 training batch 205 loss 0.22786945104599
Rank 1 training batch 210 loss 0.3497506380081177
Rank 1 training batch 215 loss 0.28404977917671204
Rank 1 training batch 220 loss 0.31585147976875305
Rank 1 training batch 225 loss 0.23670583963394165
Rank 1 training batch 230 loss 0.31162986159324646
Rank 1 training batch 235 loss 0.2531746029853821
Rank 1 training batch 240 loss 0.3075176775455475
Rank 1 training batch 245 loss 0.1743389070034027
Rank 1 training batch 250 loss 0.22896505892276764
Rank 1 training batch 255 loss 0.32260918617248535
Rank 1 training batch 260 loss 0.21591247618198395
Rank 1 training batch 265 loss 0.24240568280220032
Rank 1 training batch 270 loss 0.21584703028202057
Rank 1 training batch 275 loss 0.18389686942100525
Rank 1 training batch 280 loss 0.17995212972164154
Rank 1 training batch 285 loss 0.3129332363605499
Rank 1 training batch 290 loss 0.2807696461677551
Rank 1 training batch 295 loss 0.22085395455360413
Rank 1 training batch 300 loss 0.2761673927307129
Rank 1 training batch 305 loss 0.27257949113845825
Rank 1 training batch 310 loss 0.27750304341316223
Rank 1 training batch 315 loss 0.14432977139949799
Rank 1 training batch 320 loss 0.24459320306777954
Rank 1 training batch 325 loss 0.23690380156040192
Rank 1 training batch 330 loss 0.20352810621261597
Rank 1 training batch 335 loss 0.24510833621025085
Rank 1 training batch 340 loss 0.2877059876918793
Rank 1 training batch 345 loss 0.18534326553344727
Rank 1 training batch 350 loss 0.22080367803573608
Rank 1 training batch 355 loss 0.18324163556098938
Rank 1 training batch 360 loss 0.21516647934913635
Rank 1 training batch 365 loss 0.16466516256332397
Rank 1 training batch 370 loss 0.2123737335205078
Rank 1 training batch 375 loss 0.1831302046775818
Rank 1 training batch 380 loss 0.17806681990623474
Rank 1 training batch 385 loss 0.26613113284111023
Rank 1 training batch 390 loss 0.24380144476890564
Rank 1 training batch 395 loss 0.26726022362709045
Rank 1 training batch 400 loss 0.18165045976638794
Rank 1 training batch 405 loss 0.43377605080604553
Rank 1 training batch 410 loss 0.2737998068332672
Rank 1 training batch 415 loss 0.2882949709892273
Rank 1 training batch 420 loss 0.15667296946048737
Rank 1 training batch 425 loss 0.20267686247825623
Rank 1 training batch 430 loss 0.24352572858333588
Rank 1 training batch 435 loss 0.267208456993103
Rank 1 training batch 440 loss 0.2860328257083893
Rank 1 training batch 445 loss 0.24704664945602417
Rank 1 training batch 450 loss 0.17984412610530853
Rank 1 training batch 455 loss 0.180555060505867
Rank 1 training batch 460 loss 0.244538351893425
Rank 1 training batch 465 loss 0.16235753893852234
Rank 1 training batch 470 loss 0.2005179077386856
Rank 1 training batch 475 loss 0.22787094116210938
Rank 1 training batch 480 loss 0.1567821502685547
Rank 1 training batch 485 loss 0.1995663195848465
Rank 1 training batch 490 loss 0.2485169917345047
Rank 1 training batch 495 loss 0.21575914323329926
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9147
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.537
saving model
