/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Building train + in-distribution test data loader from mnist_rotation_five_by_nine
Building OOD test data loader from mnist_rotation_nine_by_nine
loaders done, starting training...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Worker rank 2 initializing RPC
[W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[W tensorpipe_agent.cpp:180] Failed to look up the IP address for the hostname (EAI_NONAME: unknown node or service (this error originated at tensorpipe/transport/uv/utility.cc:97)), defaulting to 127.0.0.1
Worker 2 done initializing RPC
saving as /Users/spandanmadan/saved_models/mnist_rotation_five_by_nine_world_size_5_rank_2.pt
Starting Epoch:0
Rank 2 training batch 0 loss 2.6328353881835938
Rank 2 training batch 5 loss 2.2833807468414307
Rank 2 training batch 10 loss 2.1122987270355225
Rank 2 training batch 15 loss 1.9193711280822754
Rank 2 training batch 20 loss 1.7915860414505005
Rank 2 training batch 25 loss 1.6159543991088867
Rank 2 training batch 30 loss 1.499383807182312
Rank 2 training batch 35 loss 1.4623860120773315
Rank 2 training batch 40 loss 1.4648913145065308
Rank 2 training batch 45 loss 1.3150153160095215
Rank 2 training batch 50 loss 1.3824424743652344
Rank 2 training batch 55 loss 1.1476274728775024
Rank 2 training batch 60 loss 1.2505581378936768
Rank 2 training batch 65 loss 1.153831124305725
Rank 2 training batch 70 loss 1.0954437255859375
Rank 2 training batch 75 loss 1.0647709369659424
Rank 2 training batch 80 loss 0.9461963772773743
Rank 2 training batch 85 loss 0.9898412823677063
Rank 2 training batch 90 loss 1.0117727518081665
Rank 2 training batch 95 loss 0.9510209560394287
Rank 2 training batch 100 loss 0.8713828921318054
Rank 2 training batch 105 loss 0.8427525162696838
Rank 2 training batch 110 loss 0.9106941819190979
Rank 2 training batch 115 loss 0.8428481221199036
Rank 2 training batch 120 loss 0.5924907326698303
Rank 2 training batch 125 loss 0.7131643891334534
Rank 2 training batch 130 loss 0.8143912553787231
Rank 2 training batch 135 loss 0.7207004427909851
Rank 2 training batch 140 loss 0.8155971765518188
Rank 2 training batch 145 loss 0.6018263101577759
Rank 2 training batch 150 loss 0.5885838866233826
Rank 2 training batch 155 loss 0.7282397747039795
Rank 2 training batch 160 loss 0.7027726769447327
Rank 2 training batch 165 loss 0.45110976696014404
Rank 2 training batch 170 loss 0.44262227416038513
Rank 2 training batch 175 loss 0.6041355133056641
Rank 2 training batch 180 loss 0.6147266626358032
Rank 2 training batch 185 loss 0.5828118920326233
Rank 2 training batch 190 loss 0.4390660226345062
Rank 2 training batch 195 loss 0.5411717891693115
Rank 2 training batch 200 loss 0.6138115525245667
Rank 2 training batch 205 loss 0.45059916377067566
Rank 2 training batch 210 loss 0.5720908045768738
Rank 2 training batch 215 loss 0.673153817653656
Rank 2 training batch 220 loss 0.41475680470466614
Rank 2 training batch 225 loss 0.5219441652297974
Rank 2 training batch 230 loss 0.5178185105323792
Rank 2 training batch 235 loss 0.42549842596054077
Rank 2 training batch 240 loss 0.40864673256874084
Rank 2 training batch 245 loss 0.5023186206817627
Rank 2 training batch 250 loss 0.3556026816368103
Rank 2 training batch 255 loss 0.4949219226837158
Rank 2 training batch 260 loss 0.41023263335227966
Rank 2 training batch 265 loss 0.45775681734085083
Rank 2 training batch 270 loss 0.4944224953651428
Rank 2 training batch 275 loss 0.4768035113811493
Rank 2 training batch 280 loss 0.4365355372428894
Rank 2 training batch 285 loss 0.45421355962753296
Rank 2 training batch 290 loss 0.4904405176639557
Rank 2 training batch 295 loss 0.3092713952064514
Rank 2 training batch 300 loss 0.3233576714992523
Rank 2 training batch 305 loss 0.46930256485939026
Rank 2 training batch 310 loss 0.273851603269577
Rank 2 training batch 315 loss 0.3759152293205261
Rank 2 training batch 320 loss 0.420177698135376
Rank 2 training batch 325 loss 0.29410451650619507
Rank 2 training batch 330 loss 0.34881791472435
Rank 2 training batch 335 loss 0.29745548963546753
Rank 2 training batch 340 loss 0.4549351930618286
Rank 2 training batch 345 loss 0.4176469147205353
Rank 2 training batch 350 loss 0.339650422334671
Rank 2 training batch 355 loss 0.3851311504840851
Rank 2 training batch 360 loss 0.38533860445022583
Rank 2 training batch 365 loss 0.4925745129585266
Rank 2 training batch 370 loss 0.21086722612380981
Rank 2 training batch 375 loss 0.3380502760410309
Rank 2 training batch 380 loss 0.2680538296699524
Rank 2 training batch 385 loss 0.2837754487991333
Rank 2 training batch 390 loss 0.24832631647586823
Rank 2 training batch 395 loss 0.27243244647979736
Rank 2 training batch 400 loss 0.2286410927772522
Rank 2 training batch 405 loss 0.24442316591739655
Rank 2 training batch 410 loss 0.29902538657188416
Rank 2 training batch 415 loss 0.27766484022140503
Rank 2 training batch 420 loss 0.3089347183704376
Rank 2 training batch 425 loss 0.24649591743946075
Rank 2 training batch 430 loss 0.3005053997039795
Rank 2 training batch 435 loss 0.2454054057598114
Rank 2 training batch 440 loss 0.32090866565704346
Rank 2 training batch 445 loss 0.39699551463127136
Rank 2 training batch 450 loss 0.23332330584526062
Rank 2 training batch 455 loss 0.21227112412452698
Rank 2 training batch 460 loss 0.18720696866512299
Rank 2 training batch 465 loss 0.26417505741119385
Rank 2 training batch 470 loss 0.2238578200340271
Rank 2 training batch 475 loss 0.21783332526683807
Rank 2 training batch 480 loss 0.2526620328426361
Rank 2 training batch 485 loss 0.16729393601417542
Rank 2 training batch 490 loss 0.21673190593719482
Rank 2 training batch 495 loss 0.23166437447071075
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9061
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.514
Starting Epoch:1
Rank 2 training batch 0 loss 0.29976269602775574
Rank 2 training batch 5 loss 0.2709619104862213
Rank 2 training batch 10 loss 0.20026420056819916
Rank 2 training batch 15 loss 0.16784100234508514
Rank 2 training batch 20 loss 0.17716018855571747
Rank 2 training batch 25 loss 0.25257396697998047
Rank 2 training batch 30 loss 0.22426728904247284
Rank 2 training batch 35 loss 0.23743696510791779
Rank 2 training batch 40 loss 0.16666516661643982
Rank 2 training batch 45 loss 0.23744626343250275
Rank 2 training batch 50 loss 0.22441446781158447
Rank 2 training batch 55 loss 0.22242656350135803
Rank 2 training batch 60 loss 0.14224350452423096
Rank 2 training batch 65 loss 0.2417273074388504
Rank 2 training batch 70 loss 0.29813072085380554
Rank 2 training batch 75 loss 0.15459467470645905
Rank 2 training batch 80 loss 0.20788826048374176
Rank 2 training batch 85 loss 0.1667131930589676
Rank 2 training batch 90 loss 0.22317954897880554
Rank 2 training batch 95 loss 0.1648486852645874
Rank 2 training batch 100 loss 0.20131057500839233
Rank 2 training batch 105 loss 0.142093226313591
Rank 2 training batch 110 loss 0.16553233563899994
Rank 2 training batch 115 loss 0.13892386853694916
Rank 2 training batch 120 loss 0.15173567831516266
Rank 2 training batch 125 loss 0.15689361095428467
Rank 2 training batch 130 loss 0.18855662643909454
Rank 2 training batch 135 loss 0.16023610532283783
Rank 2 training batch 140 loss 0.2243516892194748
Rank 2 training batch 145 loss 0.20766717195510864
Rank 2 training batch 150 loss 0.19864042103290558
Rank 2 training batch 155 loss 0.19997772574424744
Rank 2 training batch 160 loss 0.2999332845211029
Rank 2 training batch 165 loss 0.1827435940504074
Rank 2 training batch 170 loss 0.14478984475135803
Rank 2 training batch 175 loss 0.18348661065101624
Rank 2 training batch 180 loss 0.15300153195858002
Rank 2 training batch 185 loss 0.13303278386592865
Rank 2 training batch 190 loss 0.10577481985092163
Rank 2 training batch 195 loss 0.1822413057088852
Rank 2 training batch 200 loss 0.15383009612560272
Rank 2 training batch 205 loss 0.08858159184455872
Rank 2 training batch 210 loss 0.15627917647361755
Rank 2 training batch 215 loss 0.16525191068649292
Rank 2 training batch 220 loss 0.1341724693775177
Rank 2 training batch 225 loss 0.2345985621213913
Rank 2 training batch 230 loss 0.16014990210533142
Rank 2 training batch 235 loss 0.2198912799358368
Rank 2 training batch 240 loss 0.20226582884788513
Rank 2 training batch 245 loss 0.059960346668958664
Rank 2 training batch 250 loss 0.2845592796802521
Rank 2 training batch 255 loss 0.2775595188140869
Rank 2 training batch 260 loss 0.18383988738059998
Rank 2 training batch 265 loss 0.13807015120983124
Rank 2 training batch 270 loss 0.12050381302833557
Rank 2 training batch 275 loss 0.0981188490986824
Rank 2 training batch 280 loss 0.20459920167922974
Rank 2 training batch 285 loss 0.12217261642217636
Rank 2 training batch 290 loss 0.16643443703651428
Rank 2 training batch 295 loss 0.13297636806964874
Rank 2 training batch 300 loss 0.1471112221479416
Rank 2 training batch 305 loss 0.10203296691179276
Rank 2 training batch 310 loss 0.15473602712154388
Rank 2 training batch 315 loss 0.11494703590869904
Rank 2 training batch 320 loss 0.1012353003025055
Rank 2 training batch 325 loss 0.16751259565353394
Rank 2 training batch 330 loss 0.0781702771782875
Rank 2 training batch 335 loss 0.1259288191795349
Rank 2 training batch 340 loss 0.07634102553129196
Rank 2 training batch 345 loss 0.0844130739569664
Rank 2 training batch 350 loss 0.11795615404844284
Rank 2 training batch 355 loss 0.1633867770433426
Rank 2 training batch 360 loss 0.11843973398208618
Rank 2 training batch 365 loss 0.11153755336999893
Rank 2 training batch 370 loss 0.09622230380773544
Rank 2 training batch 375 loss 0.06467346847057343
Rank 2 training batch 380 loss 0.05058073624968529
Rank 2 training batch 385 loss 0.10732405632734299
Rank 2 training batch 390 loss 0.10478861629962921
Rank 2 training batch 395 loss 0.08841655403375626
Rank 2 training batch 400 loss 0.1652328372001648
Rank 2 training batch 405 loss 0.12011861056089401
Rank 2 training batch 410 loss 0.05818774551153183
Rank 2 training batch 415 loss 0.0989413857460022
Rank 2 training batch 420 loss 0.0869867131114006
Rank 2 training batch 425 loss 0.14118137955665588
Rank 2 training batch 430 loss 0.18493986129760742
Rank 2 training batch 435 loss 0.13930271565914154
Rank 2 training batch 440 loss 0.10668069124221802
Rank 2 training batch 445 loss 0.11596094816923141
Rank 2 training batch 450 loss 0.1086391806602478
Rank 2 training batch 455 loss 0.2150702029466629
Rank 2 training batch 460 loss 0.08452451974153519
Rank 2 training batch 465 loss 0.11908652633428574
Rank 2 training batch 470 loss 0.1049700528383255
Rank 2 training batch 475 loss 0.10083580762147903
Rank 2 training batch 480 loss 0.08439844101667404
Rank 2 training batch 485 loss 0.04430040344595909
Rank 2 training batch 490 loss 0.10013744235038757
Rank 2 training batch 495 loss 0.057670336216688156
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9448
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.5871
Starting Epoch:2
Rank 2 training batch 0 loss 0.13513362407684326
Rank 2 training batch 5 loss 0.09684772789478302
Rank 2 training batch 10 loss 0.11475375294685364
Rank 2 training batch 15 loss 0.10582201182842255
Rank 2 training batch 20 loss 0.09185819327831268
Rank 2 training batch 25 loss 0.07299365848302841
Rank 2 training batch 30 loss 0.06876715272665024
Rank 2 training batch 35 loss 0.053906071931123734
Rank 2 training batch 40 loss 0.056151267141103745
Rank 2 training batch 45 loss 0.06022427976131439
Rank 2 training batch 50 loss 0.06077248975634575
Rank 2 training batch 55 loss 0.08556761592626572
Rank 2 training batch 60 loss 0.05738149583339691
Rank 2 training batch 65 loss 0.06577759981155396
Rank 2 training batch 70 loss 0.06481373310089111
Rank 2 training batch 75 loss 0.13109588623046875
Rank 2 training batch 80 loss 0.10620443522930145
Rank 2 training batch 85 loss 0.07490814477205276
Rank 2 training batch 90 loss 0.1302911788225174
Rank 2 training batch 95 loss 0.06079145520925522
Rank 2 training batch 100 loss 0.04527941346168518
Rank 2 training batch 105 loss 0.08733806014060974
Rank 2 training batch 110 loss 0.1082175225019455
Rank 2 training batch 115 loss 0.08413801342248917
Rank 2 training batch 120 loss 0.08426013588905334
Rank 2 training batch 125 loss 0.13172437250614166
Rank 2 training batch 130 loss 0.13170555233955383
Rank 2 training batch 135 loss 0.09120658040046692
Rank 2 training batch 140 loss 0.06067445129156113
Rank 2 training batch 145 loss 0.12014028429985046
Rank 2 training batch 150 loss 0.11477949470281601
Rank 2 training batch 155 loss 0.07409384101629257
Rank 2 training batch 160 loss 0.08258833736181259
Rank 2 training batch 165 loss 0.12138107419013977
Rank 2 training batch 170 loss 0.09076081216335297
Rank 2 training batch 175 loss 0.0738440677523613
Rank 2 training batch 180 loss 0.11395625025033951
Rank 2 training batch 185 loss 0.05966080725193024
Rank 2 training batch 190 loss 0.09377323091030121
Rank 2 training batch 195 loss 0.06521960347890854
Rank 2 training batch 200 loss 0.07553554326295853
Rank 2 training batch 205 loss 0.06743434071540833
Rank 2 training batch 210 loss 0.11106017231941223
Rank 2 training batch 215 loss 0.0548723079264164
Rank 2 training batch 220 loss 0.06935735791921616
Rank 2 training batch 225 loss 0.05103164538741112
Rank 2 training batch 230 loss 0.03242334723472595
Rank 2 training batch 235 loss 0.06473025679588318
Rank 2 training batch 240 loss 0.08382359147071838
Rank 2 training batch 245 loss 0.06120477244257927
Rank 2 training batch 250 loss 0.05575377494096756
Rank 2 training batch 255 loss 0.04577461630105972
Rank 2 training batch 260 loss 0.043421968817710876
Rank 2 training batch 265 loss 0.05836750194430351
Rank 2 training batch 270 loss 0.06320378929376602
Rank 2 training batch 275 loss 0.06231050193309784
Rank 2 training batch 280 loss 0.04958951845765114
Rank 2 training batch 285 loss 0.085288867354393
Rank 2 training batch 290 loss 0.046931758522987366
Rank 2 training batch 295 loss 0.13665123283863068
Rank 2 training batch 300 loss 0.03633716329932213
Rank 2 training batch 305 loss 0.06641682237386703
Rank 2 training batch 310 loss 0.11099379509687424
Rank 2 training batch 315 loss 0.06114789471030235
Rank 2 training batch 320 loss 0.08689992874860764
Rank 2 training batch 325 loss 0.06297975778579712
Rank 2 training batch 330 loss 0.061004605144262314
Rank 2 training batch 335 loss 0.04735267534852028
Rank 2 training batch 340 loss 0.05965782701969147
Rank 2 training batch 345 loss 0.028981387615203857
Rank 2 training batch 350 loss 0.1143779307603836
Rank 2 training batch 355 loss 0.10170085728168488
Rank 2 training batch 360 loss 0.027109837159514427
Rank 2 training batch 365 loss 0.03467560186982155
Rank 2 training batch 370 loss 0.05048911273479462
Rank 2 training batch 375 loss 0.0761718824505806
Rank 2 training batch 380 loss 0.05194726586341858
Rank 2 training batch 385 loss 0.0939006432890892
Rank 2 training batch 390 loss 0.0663205161690712
Rank 2 training batch 395 loss 0.05322485789656639
Rank 2 training batch 400 loss 0.05562831833958626
Rank 2 training batch 405 loss 0.08573459833860397
Rank 2 training batch 410 loss 0.08683707565069199
Rank 2 training batch 415 loss 0.04724198207259178
Rank 2 training batch 420 loss 0.026026548817753792
Rank 2 training batch 425 loss 0.04479668289422989
Rank 2 training batch 430 loss 0.06031666323542595
Rank 2 training batch 435 loss 0.03928079456090927
Rank 2 training batch 440 loss 0.046487126499414444
Rank 2 training batch 445 loss 0.042785242199897766
Rank 2 training batch 450 loss 0.04427899420261383
Rank 2 training batch 455 loss 0.04016962647438049
Rank 2 training batch 460 loss 0.03596070781350136
Rank 2 training batch 465 loss 0.04564962536096573
Rank 2 training batch 470 loss 0.057532209903001785
Rank 2 training batch 475 loss 0.07485850155353546
Rank 2 training batch 480 loss 0.05141731724143028
Rank 2 training batch 485 loss 0.04342655837535858
Rank 2 training batch 490 loss 0.0662279948592186
Rank 2 training batch 495 loss 0.0730953961610794
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9564
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.6477
saving model
