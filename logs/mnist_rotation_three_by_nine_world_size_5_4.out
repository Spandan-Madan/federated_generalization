/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Building train + in-distribution test data loader from mnist_rotation_three_by_nine
Building OOD test data loader from mnist_rotation_nine_by_nine
loaders done, starting training...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Worker rank 4 initializing RPC
[W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[W tensorpipe_agent.cpp:180] Failed to look up the IP address for the hostname (EAI_NONAME: unknown node or service (this error originated at tensorpipe/transport/uv/utility.cc:97)), defaulting to 127.0.0.1
Worker 4 done initializing RPC
saving as /Users/spandanmadan/saved_models/mnist_rotation_three_by_nine_world_size_5_rank_4.pt
Starting Epoch:0
Rank 4 training batch 0 loss 2.5024361610412598
Rank 4 training batch 5 loss 2.199537992477417
Rank 4 training batch 10 loss 1.9755334854125977
Rank 4 training batch 15 loss 1.855006456375122
Rank 4 training batch 20 loss 1.6260864734649658
Rank 4 training batch 25 loss 1.530041217803955
Rank 4 training batch 30 loss 1.3463548421859741
Rank 4 training batch 35 loss 1.279762625694275
Rank 4 training batch 40 loss 1.1968843936920166
Rank 4 training batch 45 loss 1.050093412399292
Rank 4 training batch 50 loss 0.9465116858482361
Rank 4 training batch 55 loss 0.9375203847885132
Rank 4 training batch 60 loss 0.9273437261581421
Rank 4 training batch 65 loss 1.0421053171157837
Rank 4 training batch 70 loss 0.9348943829536438
Rank 4 training batch 75 loss 0.9749981760978699
Rank 4 training batch 80 loss 0.831184446811676
Rank 4 training batch 85 loss 0.7036615610122681
Rank 4 training batch 90 loss 0.6232425570487976
Rank 4 training batch 95 loss 0.5968765616416931
Rank 4 training batch 100 loss 0.7159584760665894
Rank 4 training batch 105 loss 0.7931440472602844
Rank 4 training batch 110 loss 0.5419309735298157
Rank 4 training batch 115 loss 0.6530028581619263
Rank 4 training batch 120 loss 0.6244314908981323
Rank 4 training batch 125 loss 0.4223178029060364
Rank 4 training batch 130 loss 0.5249197483062744
Rank 4 training batch 135 loss 0.49685412645339966
Rank 4 training batch 140 loss 0.5532248020172119
Rank 4 training batch 145 loss 0.6638097167015076
Rank 4 training batch 150 loss 0.5515828132629395
Rank 4 training batch 155 loss 0.48405349254608154
Rank 4 training batch 160 loss 0.5555052757263184
Rank 4 training batch 165 loss 0.5123428106307983
Rank 4 training batch 170 loss 0.5905841588973999
Rank 4 training batch 175 loss 0.44653987884521484
Rank 4 training batch 180 loss 0.3877778947353363
Rank 4 training batch 185 loss 0.3863218426704407
Rank 4 training batch 190 loss 0.46929070353507996
Rank 4 training batch 195 loss 0.3917735815048218
Rank 4 training batch 200 loss 0.34644031524658203
Rank 4 training batch 205 loss 0.431549608707428
Rank 4 training batch 210 loss 0.4348669946193695
Rank 4 training batch 215 loss 0.30491286516189575
Rank 4 training batch 220 loss 0.456863135099411
Rank 4 training batch 225 loss 0.3002416789531708
Rank 4 training batch 230 loss 0.4075596332550049
Rank 4 training batch 235 loss 0.32236388325691223
Rank 4 training batch 240 loss 0.34646010398864746
Rank 4 training batch 245 loss 0.4079297184944153
Rank 4 training batch 250 loss 0.36263933777809143
Rank 4 training batch 255 loss 0.303564190864563
Rank 4 training batch 260 loss 0.30314743518829346
Rank 4 training batch 265 loss 0.36259913444519043
Rank 4 training batch 270 loss 0.20437204837799072
Rank 4 training batch 275 loss 0.2622685134410858
Rank 4 training batch 280 loss 0.33602628111839294
Rank 4 training batch 285 loss 0.33694207668304443
Rank 4 training batch 290 loss 0.471261203289032
Rank 4 training batch 295 loss 0.40840935707092285
Rank 4 training batch 300 loss 0.2487424910068512
Rank 4 training batch 305 loss 0.28445133566856384
Rank 4 training batch 310 loss 0.33746325969696045
Rank 4 training batch 315 loss 0.2540741264820099
Rank 4 training batch 320 loss 0.13555565476417542
Rank 4 training batch 325 loss 0.31917205452919006
Rank 4 training batch 330 loss 0.1976081281900406
Rank 4 training batch 335 loss 0.23474019765853882
Rank 4 training batch 340 loss 0.2931102514266968
Rank 4 training batch 345 loss 0.23329441249370575
Rank 4 training batch 350 loss 0.3688480854034424
Rank 4 training batch 355 loss 0.3407178819179535
Rank 4 training batch 360 loss 0.3107592463493347
Rank 4 training batch 365 loss 0.21778365969657898
Rank 4 training batch 370 loss 0.21489955484867096
Rank 4 training batch 375 loss 0.15975189208984375
Rank 4 training batch 380 loss 0.21012583374977112
Rank 4 training batch 385 loss 0.23953920602798462
Rank 4 training batch 390 loss 0.20696797966957092
Rank 4 training batch 395 loss 0.17779214680194855
Rank 4 training batch 400 loss 0.22426916658878326
Rank 4 training batch 405 loss 0.22089241445064545
Rank 4 training batch 410 loss 0.21423903107643127
Rank 4 training batch 415 loss 0.2797582149505615
Rank 4 training batch 420 loss 0.173050194978714
Rank 4 training batch 425 loss 0.21157194674015045
Rank 4 training batch 430 loss 0.2775793671607971
Rank 4 training batch 435 loss 0.21989397704601288
Rank 4 training batch 440 loss 0.19028984010219574
Rank 4 training batch 445 loss 0.1712043434381485
Rank 4 training batch 450 loss 0.24707287549972534
Rank 4 training batch 455 loss 0.25229495763778687
Rank 4 training batch 460 loss 0.18271921575069427
Rank 4 training batch 465 loss 0.12696777284145355
Rank 4 training batch 470 loss 0.08561091125011444
Rank 4 training batch 475 loss 0.13566775619983673
Rank 4 training batch 480 loss 0.1796964854001999
Rank 4 training batch 485 loss 0.16287007927894592
Rank 4 training batch 490 loss 0.16546984016895294
Rank 4 training batch 495 loss 0.1686214655637741
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9264
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.4935
Starting Epoch:1
Rank 4 training batch 0 loss 0.20597144961357117
Rank 4 training batch 5 loss 0.12639054656028748
Rank 4 training batch 10 loss 0.1926119029521942
Rank 4 training batch 15 loss 0.11858460307121277
Rank 4 training batch 20 loss 0.1265728920698166
Rank 4 training batch 25 loss 0.11597497761249542
Rank 4 training batch 30 loss 0.14258715510368347
Rank 4 training batch 35 loss 0.15647873282432556
Rank 4 training batch 40 loss 0.15104253590106964
Rank 4 training batch 45 loss 0.09608174115419388
Rank 4 training batch 50 loss 0.16543428599834442
Rank 4 training batch 55 loss 0.14903761446475983
Rank 4 training batch 60 loss 0.14995485544204712
Rank 4 training batch 65 loss 0.21389946341514587
Rank 4 training batch 70 loss 0.11878757178783417
Rank 4 training batch 75 loss 0.09419403225183487
Rank 4 training batch 80 loss 0.13978013396263123
Rank 4 training batch 85 loss 0.1310947835445404
Rank 4 training batch 90 loss 0.11369328200817108
Rank 4 training batch 95 loss 0.16735917329788208
Rank 4 training batch 100 loss 0.08913668990135193
Rank 4 training batch 105 loss 0.09182840585708618
Rank 4 training batch 110 loss 0.07039608806371689
Rank 4 training batch 115 loss 0.11023495346307755
Rank 4 training batch 120 loss 0.2261829376220703
Rank 4 training batch 125 loss 0.1501292586326599
Rank 4 training batch 130 loss 0.17450875043869019
Rank 4 training batch 135 loss 0.10681141167879105
Rank 4 training batch 140 loss 0.08052784204483032
Rank 4 training batch 145 loss 0.19091911613941193
Rank 4 training batch 150 loss 0.15683044493198395
Rank 4 training batch 155 loss 0.12624028325080872
Rank 4 training batch 160 loss 0.12357541173696518
Rank 4 training batch 165 loss 0.04399525001645088
Rank 4 training batch 170 loss 0.08775795996189117
Rank 4 training batch 175 loss 0.16103428602218628
Rank 4 training batch 180 loss 0.10901352018117905
Rank 4 training batch 185 loss 0.16194134950637817
Rank 4 training batch 190 loss 0.11753497272729874
Rank 4 training batch 195 loss 0.1268855482339859
Rank 4 training batch 200 loss 0.11999735981225967
Rank 4 training batch 205 loss 0.23543141782283783
Rank 4 training batch 210 loss 0.08034692704677582
Rank 4 training batch 215 loss 0.1291353553533554
Rank 4 training batch 220 loss 0.11768091470003128
Rank 4 training batch 225 loss 0.1098964512348175
Rank 4 training batch 230 loss 0.07412995398044586
Rank 4 training batch 235 loss 0.05701478570699692
Rank 4 training batch 240 loss 0.10815384984016418
Rank 4 training batch 245 loss 0.12925581634044647
Rank 4 training batch 250 loss 0.0930037721991539
Rank 4 training batch 255 loss 0.12776809930801392
Rank 4 training batch 260 loss 0.09695706516504288
Rank 4 training batch 265 loss 0.17721766233444214
Rank 4 training batch 270 loss 0.08431413769721985
Rank 4 training batch 275 loss 0.12935341894626617
Rank 4 training batch 280 loss 0.12922312319278717
Rank 4 training batch 285 loss 0.1214129850268364
Rank 4 training batch 290 loss 0.12087395042181015
Rank 4 training batch 295 loss 0.09359291940927505
Rank 4 training batch 300 loss 0.11755908280611038
Rank 4 training batch 305 loss 0.0883970707654953
Rank 4 training batch 310 loss 0.098052978515625
Rank 4 training batch 315 loss 0.11338094621896744
Rank 4 training batch 320 loss 0.15515504777431488
Rank 4 training batch 325 loss 0.059394706040620804
Rank 4 training batch 330 loss 0.11431173980236053
Rank 4 training batch 335 loss 0.0764915719628334
Rank 4 training batch 340 loss 0.09403470158576965
Rank 4 training batch 345 loss 0.08205845206975937
Rank 4 training batch 350 loss 0.08464976400136948
Rank 4 training batch 355 loss 0.08860544115304947
Rank 4 training batch 360 loss 0.17710010707378387
Rank 4 training batch 365 loss 0.0699613019824028
Rank 4 training batch 370 loss 0.08377604186534882
Rank 4 training batch 375 loss 0.1032862588763237
Rank 4 training batch 380 loss 0.0581236407160759
Rank 4 training batch 385 loss 0.07362126559019089
Rank 4 training batch 390 loss 0.08422926068305969
Rank 4 training batch 395 loss 0.12387102842330933
Rank 4 training batch 400 loss 0.07722603529691696
Rank 4 training batch 405 loss 0.10592742264270782
Rank 4 training batch 410 loss 0.06688231974840164
Rank 4 training batch 415 loss 0.07468262314796448
Rank 4 training batch 420 loss 0.12973667681217194
Rank 4 training batch 425 loss 0.10069916397333145
Rank 4 training batch 430 loss 0.026212649419903755
Rank 4 training batch 435 loss 0.10434657335281372
Rank 4 training batch 440 loss 0.055785488337278366
Rank 4 training batch 445 loss 0.07616480439901352
Rank 4 training batch 450 loss 0.03833357244729996
Rank 4 training batch 455 loss 0.047413963824510574
Rank 4 training batch 460 loss 0.10491358488798141
Rank 4 training batch 465 loss 0.06940408051013947
Rank 4 training batch 470 loss 0.08004812896251678
Rank 4 training batch 475 loss 0.03019157238304615
Rank 4 training batch 480 loss 0.1282482147216797
Rank 4 training batch 485 loss 0.0937134176492691
Rank 4 training batch 490 loss 0.07207980006933212
Rank 4 training batch 495 loss 0.1464810073375702
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9558
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.5663
Starting Epoch:2
Rank 4 training batch 0 loss 0.06649111956357956
Rank 4 training batch 5 loss 0.10054893791675568
Rank 4 training batch 10 loss 0.054391320794820786
Rank 4 training batch 15 loss 0.05703675374388695
Rank 4 training batch 20 loss 0.03944807127118111
Rank 4 training batch 25 loss 0.10179032385349274
Rank 4 training batch 30 loss 0.04837150499224663
Rank 4 training batch 35 loss 0.0633668303489685
Rank 4 training batch 40 loss 0.1305471658706665
Rank 4 training batch 45 loss 0.08804287761449814
Rank 4 training batch 50 loss 0.055945221334695816
Rank 4 training batch 55 loss 0.040989913046360016
Rank 4 training batch 60 loss 0.030726883560419083
Rank 4 training batch 65 loss 0.02876332215964794
Rank 4 training batch 70 loss 0.06072864681482315
Rank 4 training batch 75 loss 0.04954105243086815
Rank 4 training batch 80 loss 0.03354686126112938
Rank 4 training batch 85 loss 0.03866226598620415
Rank 4 training batch 90 loss 0.0778757631778717
Rank 4 training batch 95 loss 0.08635084331035614
Rank 4 training batch 100 loss 0.08721804618835449
Rank 4 training batch 105 loss 0.09136894345283508
Rank 4 training batch 110 loss 0.049686189740896225
Rank 4 training batch 115 loss 0.062046922743320465
Rank 4 training batch 120 loss 0.0860796570777893
Rank 4 training batch 125 loss 0.05823664739727974
Rank 4 training batch 130 loss 0.09174763411283493
Rank 4 training batch 135 loss 0.048174962401390076
Rank 4 training batch 140 loss 0.06344220787286758
Rank 4 training batch 145 loss 0.05419206991791725
Rank 4 training batch 150 loss 0.08176881074905396
Rank 4 training batch 155 loss 0.05774166062474251
Rank 4 training batch 160 loss 0.026724614202976227
Rank 4 training batch 165 loss 0.06647711992263794
Rank 4 training batch 170 loss 0.06465807557106018
Rank 4 training batch 175 loss 0.05628194287419319
Rank 4 training batch 180 loss 0.07151816785335541
Rank 4 training batch 185 loss 0.04884916916489601
Rank 4 training batch 190 loss 0.032244157046079636
Rank 4 training batch 195 loss 0.12226749211549759
Rank 4 training batch 200 loss 0.08606880903244019
Rank 4 training batch 205 loss 0.045101240277290344
Rank 4 training batch 210 loss 0.042171165347099304
Rank 4 training batch 215 loss 0.03570051118731499
Rank 4 training batch 220 loss 0.09305479377508163
Rank 4 training batch 225 loss 0.06919068843126297
Rank 4 training batch 230 loss 0.048381056636571884
Rank 4 training batch 235 loss 0.04049472510814667
Rank 4 training batch 240 loss 0.03953513875603676
Rank 4 training batch 245 loss 0.029407808557152748
Rank 4 training batch 250 loss 0.07373996078968048
Rank 4 training batch 255 loss 0.027921540662646294
Rank 4 training batch 260 loss 0.04652759060263634
Rank 4 training batch 265 loss 0.0547676719725132
Rank 4 training batch 270 loss 0.04951930418610573
Rank 4 training batch 275 loss 0.02956177294254303
Rank 4 training batch 280 loss 0.031108791008591652
Rank 4 training batch 285 loss 0.03497267886996269
Rank 4 training batch 290 loss 0.0448654405772686
Rank 4 training batch 295 loss 0.09352609515190125
Rank 4 training batch 300 loss 0.06110142171382904
Rank 4 training batch 305 loss 0.0460689403116703
Rank 4 training batch 310 loss 0.04630130156874657
Rank 4 training batch 315 loss 0.07139091938734055
Rank 4 training batch 320 loss 0.03166729584336281
Rank 4 training batch 325 loss 0.06429000198841095
Rank 4 training batch 330 loss 0.03629641234874725
Rank 4 training batch 335 loss 0.05225171893835068
Rank 4 training batch 340 loss 0.026807954534888268
Rank 4 training batch 345 loss 0.02816631644964218
Rank 4 training batch 350 loss 0.06426843255758286
Rank 4 training batch 355 loss 0.03938669711351395
Rank 4 training batch 360 loss 0.10051818937063217
Rank 4 training batch 365 loss 0.021332817152142525
Rank 4 training batch 370 loss 0.05214940384030342
Rank 4 training batch 375 loss 0.059840716421604156
Rank 4 training batch 380 loss 0.02420620620250702
Rank 4 training batch 385 loss 0.03473904728889465
Rank 4 training batch 390 loss 0.046465810388326645
Rank 4 training batch 395 loss 0.027036316692829132
Rank 4 training batch 400 loss 0.05091122165322304
Rank 4 training batch 405 loss 0.07073049247264862
Rank 4 training batch 410 loss 0.05282549932599068
Rank 4 training batch 415 loss 0.02127096801996231
Rank 4 training batch 420 loss 0.0442962609231472
Rank 4 training batch 425 loss 0.0831831842660904
Rank 4 training batch 430 loss 0.039539143443107605
Rank 4 training batch 435 loss 0.020607881247997284
Rank 4 training batch 440 loss 0.022464828565716743
Rank 4 training batch 445 loss 0.04501025378704071
Rank 4 training batch 450 loss 0.025635061785578728
Rank 4 training batch 455 loss 0.033943548798561096
Rank 4 training batch 460 loss 0.015469101257622242
Rank 4 training batch 465 loss 0.023325692862272263
Rank 4 training batch 470 loss 0.03943349048495293
Rank 4 training batch 475 loss 0.053633712232112885
Rank 4 training batch 480 loss 0.02246115729212761
Rank 4 training batch 485 loss 0.021832801401615143
Rank 4 training batch 490 loss 0.018188852816820145
Rank 4 training batch 495 loss 0.03266768157482147
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9638
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.5946
saving model
[W tensorpipe_agent.cpp:726] RPC agent for trainer_4 encountered error when reading incoming request from parameter_server: pipe closed (this error originated at tensorpipe/core/pipe_impl.cc:356)
[W tensorpipe_agent.cpp:726] RPC agent for trainer_4 encountered error when reading incoming request from trainer_1: pipe closed (this error originated at tensorpipe/core/pipe_impl.cc:356)
Traceback (most recent call last):
  File "out_of_distribution_parameter_server.py", line 529, in <module>
    p.join()
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
Process Process-1:
Traceback (most recent call last):
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/Users/spandanmadan/federated_generalization/out_of_distribution_parameter_server.py", line 419, in run_worker
    rpc.shutdown()
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/site-packages/torch/distributed/rpc/api.py", line 82, in wrapper
    return func(*args, **kwargs)
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/site-packages/torch/distributed/rpc/api.py", line 362, in shutdown
    _wait_all_workers(timeout)
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/site-packages/torch/distributed/rpc/api.py", line 82, in wrapper
    return func(*args, **kwargs)
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/site-packages/torch/distributed/rpc/api.py", line 300, in _wait_all_workers
    _all_gather(None, timeout=timeout)
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/site-packages/torch/distributed/rpc/api.py", line 82, in wrapper
    return func(*args, **kwargs)
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/site-packages/torch/distributed/rpc/api.py", line 235, in _all_gather
    states.proceed_signal.wait(timeout=signal_timeout)
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/threading.py", line 558, in wait
    signaled = self._cond.wait(timeout)
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/threading.py", line 302, in wait
    waiter.acquire()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/Users/spandanmadan/miniconda3/envs/turing/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt
