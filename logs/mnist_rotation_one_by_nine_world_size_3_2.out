/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Building train + in-distribution test data loader from mnist_rotation_one_by_nine
Building OOD test data loader from mnist_rotation_nine_by_nine
loaders done, starting training...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Worker rank 2 initializing RPC
[W ProcessGroupGloo.cpp:724] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[W tensorpipe_agent.cpp:180] Failed to look up the IP address for the hostname (EAI_NONAME: unknown node or service (this error originated at tensorpipe/transport/uv/utility.cc:97)), defaulting to 127.0.0.1
Worker 2 done initializing RPC
saving as /Users/spandanmadan/saved_models/mnist_rotation_one_by_nine_world_size_3_rank_2.pt
Starting Epoch:0
Rank 2 training batch 0 loss 2.6176111698150635
Rank 2 training batch 5 loss 2.2303035259246826
Rank 2 training batch 10 loss 1.975906252861023
Rank 2 training batch 15 loss 1.8230891227722168
Rank 2 training batch 20 loss 1.643397569656372
Rank 2 training batch 25 loss 1.420149803161621
Rank 2 training batch 30 loss 1.2599343061447144
Rank 2 training batch 35 loss 1.1930919885635376
Rank 2 training batch 40 loss 1.01068913936615
Rank 2 training batch 45 loss 0.9470906257629395
Rank 2 training batch 50 loss 0.7901937365531921
Rank 2 training batch 55 loss 0.8643403053283691
Rank 2 training batch 60 loss 0.8147165775299072
Rank 2 training batch 65 loss 0.7163079977035522
Rank 2 training batch 70 loss 0.6891334056854248
Rank 2 training batch 75 loss 0.5848730206489563
Rank 2 training batch 80 loss 0.5613036155700684
Rank 2 training batch 85 loss 0.455997496843338
Rank 2 training batch 90 loss 0.5005279779434204
Rank 2 training batch 95 loss 0.5039017200469971
Rank 2 training batch 100 loss 0.41181090474128723
Rank 2 training batch 105 loss 0.4357745051383972
Rank 2 training batch 110 loss 0.5449598431587219
Rank 2 training batch 115 loss 0.5070449709892273
Rank 2 training batch 120 loss 0.3612140715122223
Rank 2 training batch 125 loss 0.41234076023101807
Rank 2 training batch 130 loss 0.33318302035331726
Rank 2 training batch 135 loss 0.4026735723018646
Rank 2 training batch 140 loss 0.29739850759506226
Rank 2 training batch 145 loss 0.29729774594306946
Rank 2 training batch 150 loss 0.3117796778678894
Rank 2 training batch 155 loss 0.30428799986839294
Rank 2 training batch 160 loss 0.3410731554031372
Rank 2 training batch 165 loss 0.2720324993133545
Rank 2 training batch 170 loss 0.32880809903144836
Rank 2 training batch 175 loss 0.3172290325164795
Rank 2 training batch 180 loss 0.27718105912208557
Rank 2 training batch 185 loss 0.1995055228471756
Rank 2 training batch 190 loss 0.29154112935066223
Rank 2 training batch 195 loss 0.3334486484527588
Rank 2 training batch 200 loss 0.39934390783309937
Rank 2 training batch 205 loss 0.2233150452375412
Rank 2 training batch 210 loss 0.2574814558029175
Rank 2 training batch 215 loss 0.21258579194545746
Rank 2 training batch 220 loss 0.1996208131313324
Rank 2 training batch 225 loss 0.32737433910369873
Rank 2 training batch 230 loss 0.18215899169445038
Rank 2 training batch 235 loss 0.2720690369606018
Rank 2 training batch 240 loss 0.2491154670715332
Rank 2 training batch 245 loss 0.2019352912902832
Rank 2 training batch 250 loss 0.2115592360496521
Rank 2 training batch 255 loss 0.27985331416130066
Rank 2 training batch 260 loss 0.18648789823055267
Rank 2 training batch 265 loss 0.2548453211784363
Rank 2 training batch 270 loss 0.18974699079990387
Rank 2 training batch 275 loss 0.21641790866851807
Rank 2 training batch 280 loss 0.2123824954032898
Rank 2 training batch 285 loss 0.11983294785022736
Rank 2 training batch 290 loss 0.17847926914691925
Rank 2 training batch 295 loss 0.16094112396240234
Rank 2 training batch 300 loss 0.2432045340538025
Rank 2 training batch 305 loss 0.1690625548362732
Rank 2 training batch 310 loss 0.18241684138774872
Rank 2 training batch 315 loss 0.24078041315078735
Rank 2 training batch 320 loss 0.16374021768569946
Rank 2 training batch 325 loss 0.21391761302947998
Rank 2 training batch 330 loss 0.22972680628299713
Rank 2 training batch 335 loss 0.1643674075603485
Rank 2 training batch 340 loss 0.18571344017982483
Rank 2 training batch 345 loss 0.18668965995311737
Rank 2 training batch 350 loss 0.12019294500350952
Rank 2 training batch 355 loss 0.2099672555923462
Rank 2 training batch 360 loss 0.1838691532611847
Rank 2 training batch 365 loss 0.1190631091594696
Rank 2 training batch 370 loss 0.16483734548091888
Rank 2 training batch 375 loss 0.16445651650428772
Rank 2 training batch 380 loss 0.12927907705307007
Rank 2 training batch 385 loss 0.14277315139770508
Rank 2 training batch 390 loss 0.23523737490177155
Rank 2 training batch 395 loss 0.08454865217208862
Rank 2 training batch 400 loss 0.16701260209083557
Rank 2 training batch 405 loss 0.09937682747840881
Rank 2 training batch 410 loss 0.13883285224437714
Rank 2 training batch 415 loss 0.14428892731666565
Rank 2 training batch 420 loss 0.11464625597000122
Rank 2 training batch 425 loss 0.07508418709039688
Rank 2 training batch 430 loss 0.04151267930865288
Rank 2 training batch 435 loss 0.14884990453720093
Rank 2 training batch 440 loss 0.16723504662513733
Rank 2 training batch 445 loss 0.15070071816444397
Rank 2 training batch 450 loss 0.1363574117422104
Rank 2 training batch 455 loss 0.1702965497970581
Rank 2 training batch 460 loss 0.09711538255214691
Rank 2 training batch 465 loss 0.09372811019420624
Rank 2 training batch 470 loss 0.08809569478034973
Rank 2 training batch 475 loss 0.14102965593338013
Rank 2 training batch 480 loss 0.1066688820719719
Rank 2 training batch 485 loss 0.1096896380186081
Rank 2 training batch 490 loss 0.08651507645845413
Rank 2 training batch 495 loss 0.1316247582435608
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9713
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.3735
Starting Epoch:1
Rank 2 training batch 0 loss 0.11429216712713242
Rank 2 training batch 5 loss 0.08989037573337555
Rank 2 training batch 10 loss 0.1283656656742096
Rank 2 training batch 15 loss 0.062265947461128235
Rank 2 training batch 20 loss 0.07118327915668488
Rank 2 training batch 25 loss 0.12219392508268356
Rank 2 training batch 30 loss 0.07767494022846222
Rank 2 training batch 35 loss 0.1067739948630333
Rank 2 training batch 40 loss 0.1398370862007141
Rank 2 training batch 45 loss 0.08738893270492554
Rank 2 training batch 50 loss 0.11323186755180359
Rank 2 training batch 55 loss 0.07529693841934204
Rank 2 training batch 60 loss 0.06907910108566284
Rank 2 training batch 65 loss 0.10394991934299469
Rank 2 training batch 70 loss 0.09248242527246475
Rank 2 training batch 75 loss 0.09294027090072632
Rank 2 training batch 80 loss 0.07448282092809677
Rank 2 training batch 85 loss 0.08159293234348297
Rank 2 training batch 90 loss 0.13591456413269043
Rank 2 training batch 95 loss 0.08744801580905914
Rank 2 training batch 100 loss 0.10472528636455536
Rank 2 training batch 105 loss 0.0760870948433876
Rank 2 training batch 110 loss 0.06736527383327484
Rank 2 training batch 115 loss 0.08380883187055588
Rank 2 training batch 120 loss 0.10349474847316742
Rank 2 training batch 125 loss 0.05387970805168152
Rank 2 training batch 130 loss 0.06013697758316994
Rank 2 training batch 135 loss 0.04499019309878349
Rank 2 training batch 140 loss 0.047507964074611664
Rank 2 training batch 145 loss 0.08586055040359497
Rank 2 training batch 150 loss 0.07610388845205307
Rank 2 training batch 155 loss 0.11933551728725433
Rank 2 training batch 160 loss 0.08004249632358551
Rank 2 training batch 165 loss 0.0682452917098999
Rank 2 training batch 170 loss 0.05978593975305557
Rank 2 training batch 175 loss 0.058225683867931366
Rank 2 training batch 180 loss 0.05215107277035713
Rank 2 training batch 185 loss 0.06502489745616913
Rank 2 training batch 190 loss 0.04455401748418808
Rank 2 training batch 195 loss 0.053641676902770996
Rank 2 training batch 200 loss 0.06957199424505234
Rank 2 training batch 205 loss 0.04036707058548927
Rank 2 training batch 210 loss 0.11769413203001022
Rank 2 training batch 215 loss 0.07155638188123703
Rank 2 training batch 220 loss 0.12944239377975464
Rank 2 training batch 225 loss 0.0719938576221466
Rank 2 training batch 230 loss 0.08721538633108139
Rank 2 training batch 235 loss 0.11046541482210159
Rank 2 training batch 240 loss 0.061461009085178375
Rank 2 training batch 245 loss 0.12716220319271088
Rank 2 training batch 250 loss 0.08818885684013367
Rank 2 training batch 255 loss 0.0390469990670681
Rank 2 training batch 260 loss 0.09024810045957565
Rank 2 training batch 265 loss 0.046605244278907776
Rank 2 training batch 270 loss 0.0767013281583786
Rank 2 training batch 275 loss 0.06722437590360641
Rank 2 training batch 280 loss 0.021635109558701515
Rank 2 training batch 285 loss 0.04187239706516266
Rank 2 training batch 290 loss 0.05887816473841667
Rank 2 training batch 295 loss 0.08932752907276154
Rank 2 training batch 300 loss 0.07097333669662476
Rank 2 training batch 305 loss 0.06508570164442062
Rank 2 training batch 310 loss 0.043003540486097336
Rank 2 training batch 315 loss 0.0596531443297863
Rank 2 training batch 320 loss 0.043090444058179855
Rank 2 training batch 325 loss 0.05832720175385475
Rank 2 training batch 330 loss 0.02946162410080433
Rank 2 training batch 335 loss 0.04647241532802582
Rank 2 training batch 340 loss 0.06016046926379204
Rank 2 training batch 345 loss 0.06677298247814178
Rank 2 training batch 350 loss 0.07442425936460495
Rank 2 training batch 355 loss 0.03998619690537453
Rank 2 training batch 360 loss 0.07747413963079453
Rank 2 training batch 365 loss 0.062100645154714584
Rank 2 training batch 370 loss 0.04267360642552376
Rank 2 training batch 375 loss 0.04159770533442497
Rank 2 training batch 380 loss 0.044309500604867935
Rank 2 training batch 385 loss 0.04733732342720032
Rank 2 training batch 390 loss 0.07455405592918396
Rank 2 training batch 395 loss 0.03764713183045387
Rank 2 training batch 400 loss 0.08479145914316177
Rank 2 training batch 405 loss 0.13032537698745728
Rank 2 training batch 410 loss 0.05142687261104584
Rank 2 training batch 415 loss 0.06949120759963989
Rank 2 training batch 420 loss 0.03157947212457657
Rank 2 training batch 425 loss 0.04804828763008118
Rank 2 training batch 430 loss 0.07536207884550095
Rank 2 training batch 435 loss 0.0586199089884758
Rank 2 training batch 440 loss 0.07091601192951202
Rank 2 training batch 445 loss 0.05904008820652962
Rank 2 training batch 450 loss 0.0563291497528553
Rank 2 training batch 455 loss 0.056384313851594925
Rank 2 training batch 460 loss 0.04447302967309952
Rank 2 training batch 465 loss 0.11582168936729431
Rank 2 training batch 470 loss 0.05172501876950264
Rank 2 training batch 475 loss 0.06410659104585648
Rank 2 training batch 480 loss 0.03154788911342621
Rank 2 training batch 485 loss 0.0966721624135971
Rank 2 training batch 490 loss 0.05254951864480972
Rank 2 training batch 495 loss 0.10987571626901627
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9839
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.3854
Starting Epoch:2
Rank 2 training batch 0 loss 0.04406232386827469
Rank 2 training batch 5 loss 0.02685016579926014
Rank 2 training batch 10 loss 0.05792595073580742
Rank 2 training batch 15 loss 0.09023789316415787
Rank 2 training batch 20 loss 0.0628632977604866
Rank 2 training batch 25 loss 0.05854079872369766
Rank 2 training batch 30 loss 0.03289233520627022
Rank 2 training batch 35 loss 0.04812400043010712
Rank 2 training batch 40 loss 0.05077541992068291
Rank 2 training batch 45 loss 0.036520492285490036
Rank 2 training batch 50 loss 0.04771297052502632
Rank 2 training batch 55 loss 0.053060032427310944
Rank 2 training batch 60 loss 0.03560073301196098
Rank 2 training batch 65 loss 0.06978920847177505
Rank 2 training batch 70 loss 0.04920891672372818
Rank 2 training batch 75 loss 0.0935000330209732
Rank 2 training batch 80 loss 0.04575610160827637
Rank 2 training batch 85 loss 0.03735984489321709
Rank 2 training batch 90 loss 0.018204662948846817
Rank 2 training batch 95 loss 0.08020377159118652
Rank 2 training batch 100 loss 0.030827805399894714
Rank 2 training batch 105 loss 0.033533934503793716
Rank 2 training batch 110 loss 0.050846315920352936
Rank 2 training batch 115 loss 0.07255418598651886
Rank 2 training batch 120 loss 0.023600082844495773
Rank 2 training batch 125 loss 0.04162336885929108
Rank 2 training batch 130 loss 0.03912987560033798
Rank 2 training batch 135 loss 0.020161187276244164
Rank 2 training batch 140 loss 0.02636527456343174
Rank 2 training batch 145 loss 0.04011685028672218
Rank 2 training batch 150 loss 0.029217079281806946
Rank 2 training batch 155 loss 0.03270401433110237
Rank 2 training batch 160 loss 0.02621612325310707
Rank 2 training batch 165 loss 0.06956981867551804
Rank 2 training batch 170 loss 0.020459743216633797
Rank 2 training batch 175 loss 0.05357654392719269
Rank 2 training batch 180 loss 0.022776078432798386
Rank 2 training batch 185 loss 0.026620270684361458
Rank 2 training batch 190 loss 0.018275948241353035
Rank 2 training batch 195 loss 0.05530909076333046
Rank 2 training batch 200 loss 0.08154558390378952
Rank 2 training batch 205 loss 0.061749257147312164
Rank 2 training batch 210 loss 0.06172475218772888
Rank 2 training batch 215 loss 0.04768887907266617
Rank 2 training batch 220 loss 0.07539552450180054
Rank 2 training batch 225 loss 0.018124695867300034
Rank 2 training batch 230 loss 0.018596621230244637
Rank 2 training batch 235 loss 0.04896707832813263
Rank 2 training batch 240 loss 0.020055999979376793
Rank 2 training batch 245 loss 0.036646727472543716
Rank 2 training batch 250 loss 0.031093694269657135
Rank 2 training batch 255 loss 0.060819972306489944
Rank 2 training batch 260 loss 0.029595697298645973
Rank 2 training batch 265 loss 0.018198862671852112
Rank 2 training batch 270 loss 0.018307693302631378
Rank 2 training batch 275 loss 0.05491898953914642
Rank 2 training batch 280 loss 0.027359530329704285
Rank 2 training batch 285 loss 0.03420988470315933
Rank 2 training batch 290 loss 0.02246331050992012
Rank 2 training batch 295 loss 0.025382502004504204
Rank 2 training batch 300 loss 0.06576414406299591
Rank 2 training batch 305 loss 0.03497292846441269
Rank 2 training batch 310 loss 0.031187033280730247
Rank 2 training batch 315 loss 0.013605322688817978
Rank 2 training batch 320 loss 0.019701935350894928
Rank 2 training batch 325 loss 0.06367423385381699
Rank 2 training batch 330 loss 0.04875563457608223
Rank 2 training batch 335 loss 0.02644994482398033
Rank 2 training batch 340 loss 0.034502629190683365
Rank 2 training batch 345 loss 0.09045840054750443
Rank 2 training batch 350 loss 0.04966443032026291
Rank 2 training batch 355 loss 0.028410835191607475
Rank 2 training batch 360 loss 0.06289122998714447
Rank 2 training batch 365 loss 0.023527326062321663
Rank 2 training batch 370 loss 0.02943195402622223
Rank 2 training batch 375 loss 0.026975203305482864
Rank 2 training batch 380 loss 0.030454017221927643
Rank 2 training batch 385 loss 0.0430721677839756
Rank 2 training batch 390 loss 0.04974611848592758
Rank 2 training batch 395 loss 0.019148891791701317
Rank 2 training batch 400 loss 0.03068605810403824
Rank 2 training batch 405 loss 0.04548444598913193
Rank 2 training batch 410 loss 0.011301977559924126
Rank 2 training batch 415 loss 0.032861486077308655
Rank 2 training batch 420 loss 0.03404776751995087
Rank 2 training batch 425 loss 0.035702452063560486
Rank 2 training batch 430 loss 0.023098375648260117
Rank 2 training batch 435 loss 0.010757694020867348
Rank 2 training batch 440 loss 0.09952406585216522
Rank 2 training batch 445 loss 0.04655805230140686
Rank 2 training batch 450 loss 0.05008409544825554
Rank 2 training batch 455 loss 0.019348597154021263
Rank 2 training batch 460 loss 0.03760962933301926
Rank 2 training batch 465 loss 0.007071267813444138
Rank 2 training batch 470 loss 0.03378346934914589
Rank 2 training batch 475 loss 0.034878943115472794
Rank 2 training batch 480 loss 0.0370061956346035
Rank 2 training batch 485 loss 0.02295863628387451
Rank 2 training batch 490 loss 0.016505395993590355
Rank 2 training batch 495 loss 0.050078134983778
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9878
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.3884
Starting Epoch:3
Rank 2 training batch 0 loss 0.04097679257392883
Rank 2 training batch 5 loss 0.03282557800412178
Rank 2 training batch 10 loss 0.027588052675127983
Rank 2 training batch 15 loss 0.016036588698625565
Rank 2 training batch 20 loss 0.010036656633019447
Rank 2 training batch 25 loss 0.03320005163550377
Rank 2 training batch 30 loss 0.04307559132575989
Rank 2 training batch 35 loss 0.04585668444633484
Rank 2 training batch 40 loss 0.06322485953569412
Rank 2 training batch 45 loss 0.020100919529795647
Rank 2 training batch 50 loss 0.03639541566371918
Rank 2 training batch 55 loss 0.04805796965956688
Rank 2 training batch 60 loss 0.09185267239809036
Rank 2 training batch 65 loss 0.03182785585522652
Rank 2 training batch 70 loss 0.02232293412089348
Rank 2 training batch 75 loss 0.035318925976753235
Rank 2 training batch 80 loss 0.0165058895945549
Rank 2 training batch 85 loss 0.044234614819288254
Rank 2 training batch 90 loss 0.03351074084639549
Rank 2 training batch 95 loss 0.024342356249690056
Rank 2 training batch 100 loss 0.05530566722154617
Rank 2 training batch 105 loss 0.0756172388792038
Rank 2 training batch 110 loss 0.02024111896753311
Rank 2 training batch 115 loss 0.04771097004413605
Rank 2 training batch 120 loss 0.01784641109406948
Rank 2 training batch 125 loss 0.03373132273554802
Rank 2 training batch 130 loss 0.015938643366098404
Rank 2 training batch 135 loss 0.03231000527739525
Rank 2 training batch 140 loss 0.020113401114940643
Rank 2 training batch 145 loss 0.04483316093683243
Rank 2 training batch 150 loss 0.03235384076833725
Rank 2 training batch 155 loss 0.03471619635820389
Rank 2 training batch 160 loss 0.012104738503694534
Rank 2 training batch 165 loss 0.032056644558906555
Rank 2 training batch 170 loss 0.028991781175136566
Rank 2 training batch 175 loss 0.03946492448449135
Rank 2 training batch 180 loss 0.04483046010136604
Rank 2 training batch 185 loss 0.01447161566466093
Rank 2 training batch 190 loss 0.013421619310975075
Rank 2 training batch 195 loss 0.015930810943245888
Rank 2 training batch 200 loss 0.008603871800005436
Rank 2 training batch 205 loss 0.09073526412248611
Rank 2 training batch 210 loss 0.028013059869408607
Rank 2 training batch 215 loss 0.03738480433821678
Rank 2 training batch 220 loss 0.021190445870161057
Rank 2 training batch 225 loss 0.02611515112221241
Rank 2 training batch 230 loss 0.022919561713933945
Rank 2 training batch 235 loss 0.033691175282001495
Rank 2 training batch 240 loss 0.009929877705872059
Rank 2 training batch 245 loss 0.01658129133284092
Rank 2 training batch 250 loss 0.030319811776280403
Rank 2 training batch 255 loss 0.040662359446287155
Rank 2 training batch 260 loss 0.0214613676071167
Rank 2 training batch 265 loss 0.0303657129406929
Rank 2 training batch 270 loss 0.02388586290180683
Rank 2 training batch 275 loss 0.028899075463414192
Rank 2 training batch 280 loss 0.06156638264656067
Rank 2 training batch 285 loss 0.010556378401815891
Rank 2 training batch 290 loss 0.008166991174221039
Rank 2 training batch 295 loss 0.04289130121469498
Rank 2 training batch 300 loss 0.018369389697909355
Rank 2 training batch 305 loss 0.0230775885283947
Rank 2 training batch 310 loss 0.034758590161800385
Rank 2 training batch 315 loss 0.013882195577025414
Rank 2 training batch 320 loss 0.03875299543142319
Rank 2 training batch 325 loss 0.04209848865866661
Rank 2 training batch 330 loss 0.017385954037308693
Rank 2 training batch 335 loss 0.018869483843445778
Rank 2 training batch 340 loss 0.012365174479782581
Rank 2 training batch 345 loss 0.02057068794965744
Rank 2 training batch 350 loss 0.023330647498369217
Rank 2 training batch 355 loss 0.027888989076018333
Rank 2 training batch 360 loss 0.009328010492026806
Rank 2 training batch 365 loss 0.01263477560132742
Rank 2 training batch 370 loss 0.04497215151786804
Rank 2 training batch 375 loss 0.022197511047124863
Rank 2 training batch 380 loss 0.013012783601880074
Rank 2 training batch 385 loss 0.021583206951618195
Rank 2 training batch 390 loss 0.014384125359356403
Rank 2 training batch 395 loss 0.023288486525416374
Rank 2 training batch 400 loss 0.026767823845148087
Rank 2 training batch 405 loss 0.012932704761624336
Rank 2 training batch 410 loss 0.02335360459983349
Rank 2 training batch 415 loss 0.008195760659873486
Rank 2 training batch 420 loss 0.015570618212223053
Rank 2 training batch 425 loss 0.02544221840798855
Rank 2 training batch 430 loss 0.024766379967331886
Rank 2 training batch 435 loss 0.020344171673059464
Rank 2 training batch 440 loss 0.01352557074278593
Rank 2 training batch 445 loss 0.05000732094049454
Rank 2 training batch 450 loss 0.02934379316866398
Rank 2 training batch 455 loss 0.02571166679263115
Rank 2 training batch 460 loss 0.016527855768799782
Rank 2 training batch 465 loss 0.01992608606815338
Rank 2 training batch 470 loss 0.03324440121650696
Rank 2 training batch 475 loss 0.010585112497210503
Rank 2 training batch 480 loss 0.04710308462381363
Rank 2 training batch 485 loss 0.014737525954842567
Rank 2 training batch 490 loss 0.012265658006072044
Rank 2 training batch 495 loss 0.013136843219399452
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9884
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.3922
Starting Epoch:4
Rank 2 training batch 0 loss 0.01446085050702095
Rank 2 training batch 5 loss 0.01897473819553852
Rank 2 training batch 10 loss 0.01850559562444687
Rank 2 training batch 15 loss 0.02287638746201992
Rank 2 training batch 20 loss 0.02011300064623356
Rank 2 training batch 25 loss 0.02592923864722252
Rank 2 training batch 30 loss 0.02643083967268467
Rank 2 training batch 35 loss 0.02181992679834366
Rank 2 training batch 40 loss 0.03151955455541611
Rank 2 training batch 45 loss 0.021372878924012184
Rank 2 training batch 50 loss 0.011309986002743244
Rank 2 training batch 55 loss 0.016151251271367073
Rank 2 training batch 60 loss 0.031987812370061874
Rank 2 training batch 65 loss 0.03115084208548069
Rank 2 training batch 70 loss 0.027289826422929764
Rank 2 training batch 75 loss 0.010171431116759777
Rank 2 training batch 80 loss 0.028460681438446045
Rank 2 training batch 85 loss 0.013742102310061455
Rank 2 training batch 90 loss 0.018543552607297897
Rank 2 training batch 95 loss 0.026739366352558136
Rank 2 training batch 100 loss 0.07557475566864014
Rank 2 training batch 105 loss 0.011800391599535942
Rank 2 training batch 110 loss 0.022748813033103943
Rank 2 training batch 115 loss 0.01716010272502899
Rank 2 training batch 120 loss 0.02605636790394783
Rank 2 training batch 125 loss 0.030740195885300636
Rank 2 training batch 130 loss 0.019009919837117195
Rank 2 training batch 135 loss 0.014856774359941483
Rank 2 training batch 140 loss 0.02843930944800377
Rank 2 training batch 145 loss 0.0066018677316606045
Rank 2 training batch 150 loss 0.04247912019491196
Rank 2 training batch 155 loss 0.016223810613155365
Rank 2 training batch 160 loss 0.02074950933456421
Rank 2 training batch 165 loss 0.029925312846899033
Rank 2 training batch 170 loss 0.005472163669764996
Rank 2 training batch 175 loss 0.03794685751199722
Rank 2 training batch 180 loss 0.008486313745379448
Rank 2 training batch 185 loss 0.024225926026701927
Rank 2 training batch 190 loss 0.02431539073586464
Rank 2 training batch 195 loss 0.007746838964521885
Rank 2 training batch 200 loss 0.019841166213154793
Rank 2 training batch 205 loss 0.07721416652202606
Rank 2 training batch 210 loss 0.007240785285830498
Rank 2 training batch 215 loss 0.016457844525575638
Rank 2 training batch 220 loss 0.00992762204259634
Rank 2 training batch 225 loss 0.0308407973498106
Rank 2 training batch 230 loss 0.01384767983108759
Rank 2 training batch 235 loss 0.028992395848035812
Rank 2 training batch 240 loss 0.005941220559179783
Rank 2 training batch 245 loss 0.006816355511546135
Rank 2 training batch 250 loss 0.05349431186914444
Rank 2 training batch 255 loss 0.019157445058226585
Rank 2 training batch 260 loss 0.04713040217757225
Rank 2 training batch 265 loss 0.021277256309986115
Rank 2 training batch 270 loss 0.019866272807121277
Rank 2 training batch 275 loss 0.0382741242647171
Rank 2 training batch 280 loss 0.05855019390583038
Rank 2 training batch 285 loss 0.0153106190264225
Rank 2 training batch 290 loss 0.014169510453939438
Rank 2 training batch 295 loss 0.018385931849479675
Rank 2 training batch 300 loss 0.00860338844358921
Rank 2 training batch 305 loss 0.05880791321396828
Rank 2 training batch 310 loss 0.023818116635084152
Rank 2 training batch 315 loss 0.0217260904610157
Rank 2 training batch 320 loss 0.005420592613518238
Rank 2 training batch 325 loss 0.012831552885472775
Rank 2 training batch 330 loss 0.011963672935962677
Rank 2 training batch 335 loss 0.012116883881390095
Rank 2 training batch 340 loss 0.010059907101094723
Rank 2 training batch 345 loss 0.014690551906824112
Rank 2 training batch 350 loss 0.026711666956543922
Rank 2 training batch 355 loss 0.028548385947942734
Rank 2 training batch 360 loss 0.015370497480034828
Rank 2 training batch 365 loss 0.0396324023604393
Rank 2 training batch 370 loss 0.06467916071414948
Rank 2 training batch 375 loss 0.012275888584554195
Rank 2 training batch 380 loss 0.007842973805963993
Rank 2 training batch 385 loss 0.008305185474455357
Rank 2 training batch 390 loss 0.017257701605558395
Rank 2 training batch 395 loss 0.05283552035689354
Rank 2 training batch 400 loss 0.00627180328592658
Rank 2 training batch 405 loss 0.04111843183636665
Rank 2 training batch 410 loss 0.015698887407779694
Rank 2 training batch 415 loss 0.025188708677887917
Rank 2 training batch 420 loss 0.011570025235414505
Rank 2 training batch 425 loss 0.022293532267212868
Rank 2 training batch 430 loss 0.007875368930399418
Rank 2 training batch 435 loss 0.027419956400990486
Rank 2 training batch 440 loss 0.008140992373228073
Rank 2 training batch 445 loss 0.0046761673875153065
Rank 2 training batch 450 loss 0.013487465679645538
Rank 2 training batch 455 loss 0.009235276840627193
Rank 2 training batch 460 loss 0.015179748646914959
Rank 2 training batch 465 loss 0.019630035385489464
Rank 2 training batch 470 loss 0.009177234023809433
Rank 2 training batch 475 loss 0.006410201080143452
Rank 2 training batch 480 loss 0.012735946103930473
Rank 2 training batch 485 loss 0.009609712287783623
Rank 2 training batch 490 loss 0.01933162659406662
Rank 2 training batch 495 loss 0.010641016066074371
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9907
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.3921
Starting Epoch:5
Rank 2 training batch 0 loss 0.014445977285504341
Rank 2 training batch 5 loss 0.016854383051395416
Rank 2 training batch 10 loss 0.009466167539358139
Rank 2 training batch 15 loss 0.006668359041213989
Rank 2 training batch 20 loss 0.008860885165631771
Rank 2 training batch 25 loss 0.016988197341561317
Rank 2 training batch 30 loss 0.01073682215064764
Rank 2 training batch 35 loss 0.010129442438483238
Rank 2 training batch 40 loss 0.015753164887428284
Rank 2 training batch 45 loss 0.017214925959706306
Rank 2 training batch 50 loss 0.00873318687081337
Rank 2 training batch 55 loss 0.015822328627109528
Rank 2 training batch 60 loss 0.009318056516349316
Rank 2 training batch 65 loss 0.03389686718583107
Rank 2 training batch 70 loss 0.01938772015273571
Rank 2 training batch 75 loss 0.009045042097568512
Rank 2 training batch 80 loss 0.024775462225079536
Rank 2 training batch 85 loss 0.015163863077759743
Rank 2 training batch 90 loss 0.029980992898344994
Rank 2 training batch 95 loss 0.01690070703625679
Rank 2 training batch 100 loss 0.011840419843792915
Rank 2 training batch 105 loss 0.010187813080847263
Rank 2 training batch 110 loss 0.012663645669817924
Rank 2 training batch 115 loss 0.015091552399098873
Rank 2 training batch 120 loss 0.015191376209259033
Rank 2 training batch 125 loss 0.007893680594861507
Rank 2 training batch 130 loss 0.034722186625003815
Rank 2 training batch 135 loss 0.006832940503954887
Rank 2 training batch 140 loss 0.010820175521075726
Rank 2 training batch 145 loss 0.006650132592767477
Rank 2 training batch 150 loss 0.0046197534538805485
Rank 2 training batch 155 loss 0.01901252567768097
Rank 2 training batch 160 loss 0.01614953763782978
Rank 2 training batch 165 loss 0.020234258845448494
Rank 2 training batch 170 loss 0.018498750403523445
Rank 2 training batch 175 loss 0.006597448140382767
Rank 2 training batch 180 loss 0.004920774605125189
Rank 2 training batch 185 loss 0.019750993698835373
Rank 2 training batch 190 loss 0.026890933513641357
Rank 2 training batch 195 loss 0.0529632531106472
Rank 2 training batch 200 loss 0.014113967306911945
Rank 2 training batch 205 loss 0.007333957590162754
Rank 2 training batch 210 loss 0.022906234487891197
Rank 2 training batch 215 loss 0.0050584860146045685
Rank 2 training batch 220 loss 0.012947934679687023
Rank 2 training batch 225 loss 0.008878317661583424
Rank 2 training batch 230 loss 0.011224037036299706
Rank 2 training batch 235 loss 0.03255371004343033
Rank 2 training batch 240 loss 0.01840175688266754
Rank 2 training batch 245 loss 0.018871953710913658
Rank 2 training batch 250 loss 0.010670389048755169
Rank 2 training batch 255 loss 0.007368790451437235
Rank 2 training batch 260 loss 0.004926879890263081
Rank 2 training batch 265 loss 0.019838083535432816
Rank 2 training batch 270 loss 0.0230646301060915
Rank 2 training batch 275 loss 0.040062885731458664
Rank 2 training batch 280 loss 0.009872620925307274
Rank 2 training batch 285 loss 0.009282148443162441
Rank 2 training batch 290 loss 0.021602019667625427
Rank 2 training batch 295 loss 0.023308921605348587
Rank 2 training batch 300 loss 0.01824667677283287
Rank 2 training batch 305 loss 0.010030100122094154
Rank 2 training batch 310 loss 0.024097826331853867
Rank 2 training batch 315 loss 0.010315686464309692
Rank 2 training batch 320 loss 0.00421653687953949
Rank 2 training batch 325 loss 0.009296555072069168
Rank 2 training batch 330 loss 0.0029180170968174934
Rank 2 training batch 335 loss 0.014494456350803375
Rank 2 training batch 340 loss 0.05058927461504936
Rank 2 training batch 345 loss 0.028718452900648117
Rank 2 training batch 350 loss 0.015974825248122215
Rank 2 training batch 355 loss 0.011355039663612843
Rank 2 training batch 360 loss 0.023201145231723785
Rank 2 training batch 365 loss 0.010595684871077538
Rank 2 training batch 370 loss 0.004058406222611666
Rank 2 training batch 375 loss 0.02731895260512829
Rank 2 training batch 380 loss 0.013513480313122272
Rank 2 training batch 385 loss 0.027445273473858833
Rank 2 training batch 390 loss 0.009738986380398273
Rank 2 training batch 395 loss 0.04055647552013397
Rank 2 training batch 400 loss 0.034426603466272354
Rank 2 training batch 405 loss 0.030038461089134216
Rank 2 training batch 410 loss 0.0080389603972435
Rank 2 training batch 415 loss 0.011303727515041828
Rank 2 training batch 420 loss 0.014428316615521908
Rank 2 training batch 425 loss 0.006474455818533897
Rank 2 training batch 430 loss 0.006968400906771421
Rank 2 training batch 435 loss 0.00924563966691494
Rank 2 training batch 440 loss 0.0051760501228272915
Rank 2 training batch 445 loss 0.006589062046259642
Rank 2 training batch 450 loss 0.04097620025277138
Rank 2 training batch 455 loss 0.01294651534408331
Rank 2 training batch 460 loss 0.007110707927495241
Rank 2 training batch 465 loss 0.003776080207899213
Rank 2 training batch 470 loss 0.02759486623108387
Rank 2 training batch 475 loss 0.015946198254823685
Rank 2 training batch 480 loss 0.009209342300891876
Rank 2 training batch 485 loss 0.015895646065473557
Rank 2 training batch 490 loss 0.008694966323673725
Rank 2 training batch 495 loss 0.03535477817058563
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Training complete!
Getting accuracy....
In-D accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.9902
OOD accuracy...
/Users/spandanmadan/federated_generalization
/Users/spandanmadan/federated_generalization/./res/loader/loader.py
/Users/spandanmadan/federated_generalization/./res/loader
Accuracy 0.3943
saving model
